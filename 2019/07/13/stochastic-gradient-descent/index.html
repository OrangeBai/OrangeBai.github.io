<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Stochastic Gradient Descent | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="IntroWell, if you look through top journals for machine learning fields in these days, you might notice that more than half of published papers are regarding deep neural networks. This trend, I mean,">
<meta property="og:type" content="article">
<meta property="og:title" content="Stochastic Gradient Descent">
<meta property="og:url" content="http://example.com/2019/07/13/stochastic-gradient-descent/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="IntroWell, if you look through top journals for machine learning fields in these days, you might notice that more than half of published papers are regarding deep neural networks. This trend, I mean,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-07-13T01:39:19.000Z">
<meta property="article:modified_time" content="2019-07-24T20:23:29.000Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Math">
<meta property="article:tag" content="MachineLearning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-stochastic-gradient-descent" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/07/13/stochastic-gradient-descent/" class="article-date">
  <time class="dt-published" datetime="2019-07-13T01:39:19.000Z" itemprop="datePublished">2019-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Math/">Math</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Stochastic Gradient Descent
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a><strong>Intro</strong></h2><p>Well, if you look through top journals for machine learning fields in these days, you might notice that more than half of published papers are regarding deep neural networks. This trend, I mean, the popularity of DL is just like that of SVM a decade ago. To better understand the  core ideas of different models, we should focus on details such as model structure, training method, and mathmematical proofs. In this series, I am trying to introduce some basic concepts related to artificial intellegence, esciplly deep learning. To begin with, this post discuss one of the most important and the basic algorithms: Stochastic Gradient Descent, abbreviated SGD.</p>
<h2 id="Basic-Properties"><a href="#Basic-Properties" class="headerlink" title="Basic Properties"></a><strong>Basic Properties</strong></h2><p>SGD is a powerful and widely used algorithm for optimizing an object function. In most machine learning cases, loss function, which illustrates how “bad” the current model is, is used as the object function. Thus, SGD is used to minimize the loss function and improve the model performance. The idea is first introduced by <a target="_blank" rel="noopener" href="https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586">Herbert Robbins and Sutton Monro</a> [1].</p>
<h3 id="General-form-of-object-function"><a href="#General-form-of-object-function" class="headerlink" title="General form of object function"></a><strong>General form of object function</strong></h3><p>Consider the object function $Q(w)$ has the form of a sum:</p>
<script type="math/tex; mode=display">
Q(w)= \frac{1}{n}\sum_{i=1}^{n}Q_i(w),</script><p>where $w$ is the parameter from the model. In most cases, the object is to find the optimal parameter so that the value of loss function can be minimized. $Q_i(w)$ is the loss of instance $i$ with parameter $w$. For a defined and differentiable multi-variable function, the value of $F(a)$ increases the fastest when the independent vector $a$ going along the gradient direction. To find the global minimum of the function, one take steps proportional to the negative of the gradient of the function at the current point. The object function should have smoothness preperties, such as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Differentiable_function">differentiable</a> or <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Subgradient_method">subdifferentiable</a>, otherwise the “direction” then cannot be found.</p>
<h3 id="Gradient-Descent-and-Stochastic-Gradient-Descent"><a href="#Gradient-Descent-and-Stochastic-Gradient-Descent" class="headerlink" title="Gradient Descent and Stochastic Gradient Descent"></a><strong>Gradient Descent and Stochastic Gradient Descent</strong></h3><p>Intuiatively, one would like to calculate the gradient of the regarding the current parameter and move “a little step forward” so that the $Q(w)$ would reduce a little bit. The process could be described as:</p>
<script type="math/tex; mode=display">
w= w- \eta\nabla Q(w)= w - \eta\sum_{i=1}^{n}\nabla Q_i(w)/n,</script><p>where $\eta$ is the step size, also known as learning rate; $\nabla Q(w)$ is the gradient of the object function at $w$.</p>
<p>Imagine someone is at the top of a mountain and looking for a way to the valley bottom. A possible solution is that moving to the steepest direction in every step. That is what happened during a gradient descent algorithm. However, there might be many “pits” on the way down. It is possible that someone recognize a bottom of a pit as the bottom of valley when he has limited sights, since every direction seems upward. </p>
<p>A sample solution to this issue is to add some “randomness” to the algorithm. In SGD, only one or a subset of the training sample are selected for updating parameter in a particular iteration. The gradient of such a subset might not be the same as that of the whole sample, but it also leads a downward path.</p>
<p>In summary, both gradient descent and stochastic gradient descent, a set of parameters are updated in an iterative amnner to minimize and error function. The difference is, GD are running though whe complete data set while SGD only use one training sample. In general, SGD often converges much faster than GD but the error function is not as minimized as in the case of GD. Also, the path of SGD wonder more places and is less likely to be trapped into a local minimum.</p>
<p>A simple example is given in the following section.</p>
<h2 id="A-simple-Example"><a href="#A-simple-Example" class="headerlink" title="A simple Example"></a><strong>A simple Example</strong></h2><p>Let’s take a linear regression problem as an example. Denote the training set as $(\{x_1, y_1\},\{x_2, y_2\}, \dots, \{x_n, y_n\})$, where $x_i$ is independent variable and $y_i$ is dependent variable. Assume $x$ and $y$ have a linear relationship: </p>
<script type="math/tex; mode=display">
y = w_1 + w_2 x,</script><p>and the loss function, or objective function, to be minimized is:</p>
<script type="math/tex; mode=display">
Q(w) = \sum_{i=1}^{n}Q_i(w) = \sum_{i=1}^{n}(\hat{y_{i}} - y_{i})^2</script><p>where $\hat{y_{i}}$ is the estimated value of dependent variable under current parameters. When a set of parameter $(w_0, w_1)$ is randomly initiated, $y_i$ can be estimated according to the current model. Thus, in GD algorithm, an update interation is:</p>
<script type="math/tex; mode=display">
\left[
\begin{aligned}
w_1^{new} \\
w_2^{new} \\
\end{aligned}
\right] 
=
\left[
\begin{aligned}
w_1^{old} \\
w_2^{old} \\
\end{aligned}
\right] -   \eta 
\left[
    \begin{aligned}
    \sum_{i=1}^n\frac{\partial}{\partial w_2}(w_1^{old} + w_2^{old}x_i -y_i)^2 \\
    \sum_{i=1}^n\frac{\partial}{\partial w_2}(w_1^{old} + w_2^{old}x_i -y_i)^2 \\
    \end{aligned}
\right] = \left[
\begin{aligned}
w_1^{old} \\
w_2^{old} \\
\end{aligned}
\right] - \eta
\left[
    \begin{aligned}
    &\sum_{i=1}^n2(w_1^{old} + w_2^{old}x_i -y_i) \\
    &\sum_{i=1}^n2x_i(w_1^{old} + w_2^{old}x_i -y_i) \\
    \end{aligned}
\right].</script><p>For the stocastic gradient descent, an update interation can be written as:</p>
<script type="math/tex; mode=display">
\left[
\begin{aligned}
w_1^{new} \\
w_2^{new} \\
\end{aligned}
\right] 
=
\left[
\begin{aligned}
w_1^{old} \\
w_2^{old} \\
\end{aligned}
\right] -   \eta 
\left[
    \begin{aligned}
    \frac{\partial}{\partial w_2}(w_1^{old} + w_2^{old}x_i -y_i)^2 \\
    \frac{\partial}{\partial w_2}(w_1^{old} + w_2^{old}x_i -y_i)^2 \\
    \end{aligned}
\right] = \left[
\begin{aligned}
w_1^{old} \\
w_2^{old} \\
\end{aligned}
\right] - \eta
\left[
    \begin{aligned}
    &2(w_1^{old} + w_2^{old}x_i -y_i) \\
    &2x_i(w_1^{old} + w_2^{old}x_i -y_i) \\
    \end{aligned}
\right].</script><p>Note that in GD, Samples of the complete dataset are token into account for an interation, while in SGD the gradient is only evaluated at a single point $x_i$ instead of at the set of all samples. </p>
<h2 id="Extensions-and-variants"><a href="#Extensions-and-variants" class="headerlink" title="Extensions and variants"></a>Extensions and variants</h2><p>There are many improvements to the basic SGD algorithm. Most algorithms are proposed for finding a “suitable” learning rate at different scenarios. If the learning rate is fixed, the algorithm might diverge when the parameter is too high, while setting it too low makes it slow to converge. The following section presents an introduction to some of the most commonly used optimizers.</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>The momentum method is one of the earlies proposals, which is introduced by <a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">Rumelhart, Hinton, and Williams</a> [2] [3]. Given an objective function $f(\theta)$ to be minimized, classical momentum is given by:</p>
<script type="math/tex; mode=display">
\Delta w_{t+1} = \alpha \Delta w - \eta \nabla Q_i(w), 
w = w + \Delta w</script><p>where $\Delta w$ is the update at each iteration, $\eta &gt; 0$ is the learning rate, $\alpha \in [0,1]$ is the momentum coefficient, and $\nabla Q_i(\theta)$ is the gradient at $w$. Since direction $d$ of low-curvature have, according to the definition of the gradient, a slower local change in their rate of reduction (i.e., $d^T \Delta Q_i$), they will tend to persist across iterations and be amplified. <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/0041555364901375">Polyak(1964)</a> [4] showed that classic momentum algorithm can considerably accelerate convergence to a local minimum, requiring $\sqrt R$ times fewer iteration than gradient descent to reach the same level of accuracy, where $R$ is the condition number of the curvature at the minimum.</p>
<p>Intuitively, the momentum method is similar to that in physics: if the curve of an objective function at point $w$ is steep enough, the previous update might be large enough, makes the momentum, in terms of $\alpha \Delta$ amplify the following update.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><strong>Reference</strong></h2><p>[1] Robbins, H., &amp; Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, 400-407.<br>[2] Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1988). Learning representations by back-propagating errors. Cognitive modeling, 5(3), 1.<br>[3] Sutskever, I., Martens, J., Dahl, G., &amp; Hinton, G. (2013, February). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).<br>[4] Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5), 1-17.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/07/13/stochastic-gradient-descent/" data-id="cllvc8mao0000p8vf3j97ebny" data-title="Stochastic Gradient Descent" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MachineLearning/" rel="tag">MachineLearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/28/hello-world/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Hello World
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/MachineLearning/" rel="tag">MachineLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/MachineLearning/" style="font-size: 10px;">MachineLearning</a> <a href="/tags/Math/" style="font-size: 10px;">Math</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/08/28/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2019/07/13/stochastic-gradient-descent/">Stochastic Gradient Descent</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>